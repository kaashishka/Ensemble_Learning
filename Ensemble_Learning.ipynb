{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "********** Ensemble Learning ***************"
      ],
      "metadata": {
        "id": "iW3iYABu3six"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "- Answer: Ensemble Learning is a machine learning paradigm where multiple models, often called \"weak learners,\" are trained to solve the same problem and combined to get better performance. The key idea is that a group of weak models can come together to form a strong model, reducing variance (bagging), bias (boosting), or improving predictions (stacking).\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "Answer:\n",
        "\n",
        "Feature:\n",
        "Objective\n",
        "Model Training\n",
        "Sample Weight\n",
        "Example\n",
        "Overfitting\n",
        "\n",
        "Bagging:\n",
        "Reduce variance\n",
        "Independent parallel training\n",
        "Equal weights\n",
        "Random Forest\n",
        "Less prone\n",
        "\n",
        "Boosting:\n",
        "Reduce bias\n",
        "Sequential training\n",
        "Adjusted based on errors\n",
        "AdaBoost, Gradient Boosting\n",
        "More prone (but can be controlled)\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "- Answer: Bootstrap sampling is a statistical technique where samples are drawn with replacement from the original dataset. In Bagging methods like Random Forest, bootstrap sampling allows each base learner to train on a different subset of the data, promoting diversity among models and reducing overfitting.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "- Answer: OOB samples are data points not included in a particular bootstrap sample. In Random Forests, these samples are used to validate the model without needing a separate validation set. The OOB score is the average prediction accuracy on these samples and serves as an internal cross-validation metric.\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        " - Answer:\n",
        "\n",
        "Aspect:\n",
        "Stability\n",
        "Bias\n",
        "Feature Importance\n",
        "Interpretability\n",
        "\n",
        "Decision Tree:\n",
        "Unstable (high variance)\n",
        "High\n",
        "Based on single tree splits\n",
        "Easier\n",
        "\n",
        "Random Forest:\n",
        "More stable (averaged over trees)\n",
        "Lower due to ensemble averaging\n",
        "Averaged over multiple trees\n",
        "Harder due to multiple trees\n",
        "\n"
      ],
      "metadata": {
        "id": "sRzKn7hq3x-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#● Load the Breast Cancer dataset using\n",
        "#sklearn.datasets.load_breast_cancer()\n",
        "#● Train a Random Forest Classifier\n",
        "#● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "#- Answer:\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "importances = model.feature_importances_\n",
        "features = pd.Series(importances, index=data.feature_names)\n",
        "top_features = features.sort_values(ascending=False).head(5)\n",
        "print(top_features)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNauCSIf5daY",
        "outputId": "8784a8a0-7cf9-4733-97b3-2304d52eb5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "#● Evaluate its accuracy and compare with a single Decision Tree\n",
        "#- Answer:\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree classifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_predictions = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "\n",
        "# Print the accuracy of both models\n",
        "print(f\"Accuracy of Single Decision Tree: {dt_accuracy:.4f}\")\n",
        "print(f\"Accuracy of Bagging Classifier: {bagging_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti7DZlnn5paF",
        "outputId": "7750af8a-91c4-442a-a69e-e3dbbb399683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0000\n",
            "Accuracy of Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Train a Random Forest Classifier\n",
        "#● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "#● Print the best parameters and final accuracy\n",
        "#- Answer:\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [3, 5, None]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
        "grid.fit(X, y)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtmYivFR5s3t",
        "outputId": "4b81edeb-118e-45dc-c603-49bc7377bb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'n_estimators': 50}\n",
            "Best Accuracy: 0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "#Housing dataset\n",
        "#● Compare their Mean Squared Errors (MSE)\n",
        "#- Answer:\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Bagging Regressor\n",
        "bag = BaggingRegressor(random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_mse = mean_squared_error(y_test, bag.predict(X_test))\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
        "\n",
        "print(f\"Bagging MSE: {bag_mse}\")\n",
        "print(f\"Random Forest MSE: {rf_mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INijjrTk5u9D",
        "outputId": "6e42ce64-e55e-4969-f7ed-c09d72c750e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.27872374841230696\n",
            "Random Forest MSE: 0.2542358390056568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        " - Answer:\n",
        "Choose between Bagging or Boosting:\n",
        "Use Boosting (e.g., XGBoost) to handle complex patterns and reduce bias.\n",
        "\n",
        "Handle Overfitting:\n",
        "\n",
        "Use regularization (e.g., learning_rate, max_depth)\n",
        "Apply early stopping\n",
        "Use cross-validation\n",
        "Select Base Models:\n",
        "\n",
        "Decision Trees for Boosting\n",
        "Logistic Regression or SVMs for Stacking\n",
        "Evaluate Performance:\n",
        "\n",
        "Use k-fold cross-validation\n",
        "Metrics: AUC-ROC, Precision-Recall, F1-score\n",
        "Justification:\n",
        "Ensemble methods combine multiple models to improve generalization, reduce overfitting, and increase robustness. In financial applications, this leads to more accurate risk assessments and better decision-making."
      ],
      "metadata": {
        "id": "MKwKmfqV6JIw"
      }
    }
  ]
}